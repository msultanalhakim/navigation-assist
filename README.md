# 🦾 Object Detection for the Visually Impaired  

A real-time **YOLOv11-based object detection system** integrated into an **Android application** to assist visually impaired users in navigating their surroundings.  
The system detects key objects such as **doors, stairs, and obstacles**, and provides **audio feedback** to enhance user awareness and safety.  

---

## 🚀 Project Overview  
This project combines **computer vision** and **assistive technology** to empower individuals with visual impairments.  
By leveraging the YOLOv11 model, the system performs **fast and accurate object detection** in real time.  
The Android application connects with a TensorFlow Lite–optimized model to ensure smooth performance across different devices while maintaining high detection accuracy.  

---

## 🧠 Key Features  
- **Real-Time Detection** — Detects doors, stairs, people, and obstacles through the mobile camera.  
- **Audio Feedback System** — Converts detection results into spoken output to guide users.  
- **Mobile Optimization** — Ensures smooth operation on a wide range of Android devices.  
- **Accessible Design** — Simple interface with clear visual contrast and voice cues.  

---

## ⚙️ Tech Stack  
- **Model:** YOLOv11 (Ultralytics)  
- **Frameworks:** TensorFlow Lite, PyTorch  
- **Languages:** Python, Kotlin (Android)  
- **Tools:** OpenCV, Android Studio, Gradle  
- **Platform:** Android  

---

## 🔬 Workflow  
1. **Dataset Preparation** — Custom dataset with labeled objects (doors, stairs, obstacles, people).  
2. **Model Training & Fine-Tuning** — Trained YOLOv11 for real-time detection accuracy.  
3. **Model Conversion** — Exported model to TensorFlow Lite for mobile deployment.  
4. **Android Integration** — Implemented camera stream, detection logic, and text-to-speech.  
5. **Testing & Optimization** — Improving stability, latency, and detection precision.  

---

## 🎯 Goals  
- Improve **independent mobility** for visually impaired users.  
- Build a **lightweight and reliable** AI model for mobile deployment.  
- Lay the foundation for **future AR-based navigation** and **depth mapping**.  


---

## 📚 Future Improvements  
- Integrate AR depth sensing for spatial awareness.  
- Add multi-language voice support for accessibility.  
- Deploy cloud-based model performance tracking.  

---


## 💡 Author  
**Muhammad Sultan Alhakim**  
Undergraduate Student, Informatics Engineering  
Jenderal Soedirman University  
📫 [LinkedIn](https://linkedin.com/in/msultanalhakim) | [GitHub](https://github.com/msultanalhakim)
