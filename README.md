# ğŸ¦¾ Object Detection for the Visually Impaired  

A real-time **YOLOv11-based object detection system** integrated into an **Android application** to assist visually impaired users in navigating their surroundings.  
The system detects key objects such as **doors, stairs, and obstacles**, and provides **audio feedback** to enhance user awareness and safety.  

---

## ğŸš€ Project Overview  
This project combines **computer vision** and **assistive technology** to empower individuals with visual impairments.  
By leveraging the YOLOv11 model, the system performs **fast and accurate object detection** in real time.  
The Android application connects with a TensorFlow Liteâ€“optimized model to ensure smooth performance across different devices while maintaining high detection accuracy.  

---

## ğŸ§  Key Features  
- **Real-Time Detection** â€” Detects doors, stairs, people, and obstacles through the mobile camera.  
- **Audio Feedback System** â€” Converts detection results into spoken output to guide users.  
- **Mobile Optimization** â€” Ensures smooth operation on a wide range of Android devices.  
- **Accessible Design** â€” Simple interface with clear visual contrast and voice cues.  

---

## âš™ï¸ Tech Stack  
- **Model:** YOLOv11 (Ultralytics)  
- **Frameworks:** TensorFlow Lite, PyTorch  
- **Languages:** Python, Kotlin (Android)  
- **Tools:** OpenCV, Android Studio, Gradle  
- **Platform:** Android  

---

## ğŸ”¬ Workflow  
1. **Dataset Preparation** â€” Custom dataset with labeled objects (doors, stairs, obstacles, people).  
2. **Model Training & Fine-Tuning** â€” Trained YOLOv11 for real-time detection accuracy.  
3. **Model Conversion** â€” Exported model to TensorFlow Lite for mobile deployment.  
4. **Android Integration** â€” Implemented camera stream, detection logic, and text-to-speech.  
5. **Testing & Optimization** â€” Improving stability, latency, and detection precision.  

---

## ğŸ¯ Goals  
- Improve **independent mobility** for visually impaired users.  
- Build a **lightweight and reliable** AI model for mobile deployment.  
- Lay the foundation for **future AR-based navigation** and **depth mapping**.  


---

## ğŸ“š Future Improvements  
- Integrate AR depth sensing for spatial awareness.  
- Add multi-language voice support for accessibility.  
- Deploy cloud-based model performance tracking.  

---


## ğŸ’¡ Author  
**Muhammad Sultan Alhakim**  
Undergraduate Student, Informatics Engineering  
Jenderal Soedirman University  
ğŸ“« [LinkedIn](https://linkedin.com/in/msultanalhakim) | [GitHub](https://github.com/msultanalhakim)
